{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\" align=\"center\"> Projet d'introduction au Machine Learning <br/> Out-Of-Domain PoS Tagging <br/> Présentation des résultats algorithmique</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization...\n",
      "\tPreparing corpus fr.foot.test.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.gsd.test.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.sequoia.train.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.ftb.test.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.ftb.dev.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.spoken.test.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.pud.train.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.spoken.train.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.natdis.test.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.partut.dev.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.sequoia.test.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.gsd.dev.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.sequoia.dev.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.gsd.train.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.pud.test.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.ftb.train.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.partut.test.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.partut.train.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.spoken.dev.json...\n",
      "\tDone\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#chargement des corpus en mémoire\n",
    "\n",
    "import dataAnalysis as da\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "da.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistique des corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#itère sur chacun des corpus\n",
    "def iterateCorpus(f, printing = True):\n",
    "    for nameCorpus, corpus in da.listeCorpus.items():\n",
    "        if printing:\n",
    "            print(\"corpus : \", nameCorpus)\n",
    "        for typeDS, dataset in corpus.getDataset().items():\n",
    "            if printing:\n",
    "                print(\"ensemble \", typeDS)\n",
    "            f(dataset, nameCorpus, typeDS)\n",
    "\n",
    "# met à jour les données statistiques\n",
    "def updateStat(dataset, nm, tds):\n",
    "    dataset.updateStat()\n",
    "\n",
    "# affiche les statistiques basique\n",
    "\n",
    "\n",
    "def printBasicStat(dataset, nm, tds):\n",
    "    #baseStat.append((nm + \" \" + tds, len(dataset.data), dataset.nbWord, dataset.nbUniqueWords))\n",
    "    print(\"   nombre de phrase : \", len(dataset.data))\n",
    "    print(\"   nombre de mot : \", dataset.nbWord)\n",
    "    print(\"   nombre de mot unique : \", dataset.nbUniqueWords)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# affiche les 10 mots / labels les plus fréquents de chaque corpus\n",
    "def printMostFrequent(dataset, nm, tds):\n",
    "    print()\n",
    "    print(\"10 mots les plus fréquents : \")\n",
    "    print(dataset.mostFrequentWord)\n",
    "    print()\n",
    "    print()\n",
    "    print(\"labels les plus fréquents : \")\n",
    "    print(dataset.mostFrequentLabel)\n",
    "    print()\n",
    "    \n",
    "# affiche les 10 mots les plus ambigu de chaque corpus\n",
    "def printMostAmbiguousWord(dataset, nm, tds):\n",
    "    ambiguousDict, ambiguousWord = dataset.ambiguousWord()\n",
    "    \n",
    "    def sortSecond(val):\n",
    "        return len(val[1])\n",
    "    \n",
    "    ambiguousWord.sort(key = sortSecond, reverse = True)\n",
    "    \n",
    "    print()\n",
    "    print(\"5 mots les plus ambigu : \", ambiguousWord[:5])\n",
    "    print()\n",
    "    \n",
    "iterateCorpus(updateStat, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseStat = []\n",
    "\n",
    "iterateCorpus(printBasicStat)\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "#print(tabulate(baseStat, tablefmt=\"latex\", floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterateCorpus(printMostFrequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mot les plus ambigu de chaque corpus\n",
    "\n",
    "iterateCorpus(printMostAmbiguousWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out Of Vocabulary Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calcul des Out of Vocabulary Words\n",
    "oovLatex = []\n",
    "\n",
    "\n",
    "footTest = da.listeCorpus['foot'].testDataSet\n",
    "natdisTest = da.listeCorpus['natdis'].testDataSet\n",
    "\n",
    "\n",
    "for nameCorpus, corpus in da.listeCorpus.items():\n",
    "    if corpus.trainExist:\n",
    "        oovResult = corpus.computeCorpusOOV()\n",
    "        oovResultFoot = corpus.computeOOV(footTest)\n",
    "        oovResultnatdis = corpus.computeOOV(natdisTest)\n",
    "        print(nameCorpus, \" : \")\n",
    "        print(\"     Pourcentage de l'oov entre train et test : \", oovResult[0])\n",
    "        print(\"     Pourcentage de l'oov entre train et dev : \", oovResult[1])\n",
    "        print(\"     Pourcentage de l'oov entre train et footTest : \", oovResultFoot)\n",
    "        print(\"     Pourcentage de l'oov entre train et natdisTest : \", oovResultnatdis)\n",
    "        latexRows = (nameCorpus, oovResult[0], oovResult[1], oovResultFoot, oovResultnatdis)\n",
    "        oovLatex.append(latexRows)\n",
    "\n",
    "print(tabulate(oovLatex, tablefmt=\"latex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divergence de Kullback-Leibler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calcul des divergence de KullBack-Leibler\n",
    "\n",
    "latexDKL = []\n",
    "\n",
    "for nameCorpus, corpus in da.listeCorpus.items():\n",
    "    if corpus.trainExist:\n",
    "        print(nameCorpus + \" : \")\n",
    "        DKLResult = corpus.computeCorpusKLDivergence()\n",
    "        DKLFootresult = corpus.computeKLDivergence(footTest)\n",
    "        DKLnatdisResult = corpus.computeKLDivergence(natdisTest)\n",
    "        print(\"     Dkl(test||train) = \", DKLResult)\n",
    "        print(\"     Dkl(footTest||train) = \", DKLFootresult)\n",
    "        print(\"     Dkl(natdisTest||train) = \", DKLnatdisResult)\n",
    "        latexDKL.append( (nameCorpus, DKLResult, DKLFootresult, DKLnatdisResult) )\n",
    "        \n",
    "        \n",
    "print(tabulate(latexDKL, tablefmt=\"latex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latexPerplexity = []\n",
    "\n",
    "print(\"                         train,                   test,              dev\")\n",
    "for nameCorpus, corpus in da.listeCorpus.items():\n",
    "    print(nameCorpus, \" : \")\n",
    "    pp_res = corpus.computePerplexityCorpus()\n",
    "    print(\"     perplexity : \", pp_res)\n",
    "    latexRow = []\n",
    "    latexRow.append(nameCorpus)\n",
    "    if type(pp_res) == tuple:\n",
    "        for i in pp_res:\n",
    "            latexRow.append(i)\n",
    "    else:\n",
    "        latexRow.append(pp_res)\n",
    "    latexPerplexity.append( latexRow )\n",
    "    \n",
    "print(tabulate(latexPerplexity, tablefmt=\"latex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model\n",
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modelAnalysis as ma\n",
    "import DecisionTreeClassifier as dtree\n",
    "import features as ft\n",
    "import numpy as np\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, testSet = da.listeCorpus[\"partut\"].trainDataSet, da.listeCorpus[\"partut\"].testDataSet\n",
    "\n",
    "Xtrain, Ytrain = ft.buildFeature(trainSet)\n",
    "Xtest, Ytest = ft.buildFeature(testSet)\n",
    "\n",
    "decisionTree = dtree.DecisionTreeClassifier(Xtrain, Ytrain, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTree = dtree.DecisionTreeClassifier(Xtrain, Ytrain, max_depth=16, split_criterion=\"gini\")\n",
    "t = time.time()\n",
    "tree = decisionTree.fit()\n",
    "print(10, \" \", time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, cm = decisionTree.modelScore(tree, Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bestTree, bestParam, scoreDev = ma.analyzeDecisionTree(\"partut\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for namedataset, _ in da.listeCorpus.items():\n",
    "    ma.analyzeDecisionTree(namedataset, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for namedataset, _ in da.listeCorpus.items():\n",
    "    for test in [\"foot\", \"natdis\"]:\n",
    "        ma.analyzeDecisionTree(namedataset, nameTestSet=test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bestParam)\n",
    "print(scoreDev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NaiveBayesClassifier as naivebc\n",
    "\n",
    "naiveBC = naivebc.NaiveBayesClassifier(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "naiveBC.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, cm = naiveBC.modelScore(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma.showConfusionMatrix(cm, decisionTree.classLabel.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xtrain, Ytrain = ft.buildFeature(trainSet, lambda x:x)\n",
    "Xtest, Ytest = ft.buildFeature(testSet, lambda x:x)\n",
    "\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(trainSet, testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
