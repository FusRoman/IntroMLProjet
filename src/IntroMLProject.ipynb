{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\" align=\"center\"> Projet d'introduction au Machine Learning <br/> Out-Of-Domain PoS Tagging <br/> Présentation des résultats algorithmique</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization...\n",
      "\tPreparing corpus fr.foot.test.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.gsd.test.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.sequoia.train.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.ftb.test.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.ftb.dev.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.spoken.test.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.pud.train.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.spoken.train.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.natdis.test.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.partut.dev.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.sequoia.test.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.gsd.dev.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.sequoia.dev.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.gsd.train.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.pud.test.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.ftb.train.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.partut.test.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.partut.train.json...\n",
      "\tDone\n",
      "\tPreparing corpus fr.spoken.dev.json...\n",
      "\tDone\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#chargement des corpus en mémoire\n",
    "\n",
    "import dataAnalysis as da\n",
    "import pandas as pds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pds.set_option('display.max_colwidth', -1)\n",
    "da.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistique des corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#itère sur chacun des corpus\n",
    "def iterateCorpus(f, printing = True):\n",
    "    for nameCorpus, corpus in da.listeCorpus.items():\n",
    "        if printing:\n",
    "            print(\"corpus : \", nameCorpus)\n",
    "        for typeDS, dataset in corpus.getDataset().items():\n",
    "            if printing:\n",
    "                print(\"ensemble \", typeDS)\n",
    "            f(dataset, nameCorpus, typeDS)\n",
    "\n",
    "# met à jour les données statistiques\n",
    "def updateStat(dataset, nm, tds):\n",
    "    dataset.updateStat()\n",
    "\n",
    "# affiche les statistiques basique\n",
    "\n",
    "\n",
    "def printBasicStat(dataset, nm, tds):\n",
    "    #baseStat.append((nm + \" \" + tds, len(dataset.data), dataset.nbWord, dataset.nbUniqueWords))\n",
    "    print(\"   nombre de phrase : \", len(dataset.data))\n",
    "    print(\"   nombre de mot : \", dataset.nbWord)\n",
    "    print(\"   nombre de mot unique : \", dataset.nbUniqueWords)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# affiche les 10 mots / labels les plus fréquents de chaque corpus\n",
    "def printMostFrequent(dataset, nm, tds):\n",
    "    print()\n",
    "    print(\"10 mots les plus fréquents : \")\n",
    "    print(dataset.mostFrequentWord)\n",
    "    print()\n",
    "    print()\n",
    "    print(\"labels les plus fréquents : \")\n",
    "    print(dataset.mostFrequentLabel)\n",
    "    print()\n",
    "    \n",
    "# affiche les 10 mots les plus ambigu de chaque corpus\n",
    "def printMostAmbiguousWord(dataset, nm, tds):\n",
    "    ambiguousDict, ambiguousWord = dataset.ambiguousWord()\n",
    "    \n",
    "    def sortSecond(val):\n",
    "        return len(val[1])\n",
    "    \n",
    "    ambiguousWord.sort(key = sortSecond, reverse = True)\n",
    "    \n",
    "    print()\n",
    "    print(\"5 mots les plus ambigu : \", ambiguousWord[:5])\n",
    "    print()\n",
    "    \n",
    "iterateCorpus(updateStat, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseStat = []\n",
    "\n",
    "iterateCorpus(printBasicStat)\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "#print(tabulate(baseStat, tablefmt=\"latex\", floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterateCorpus(printMostFrequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mot les plus ambigu de chaque corpus\n",
    "\n",
    "iterateCorpus(printMostAmbiguousWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out Of Vocabulary Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calcul des Out of Vocabulary Words\n",
    "oovLatex = []\n",
    "\n",
    "\n",
    "footTest = da.listeCorpus['foot'].testDataSet\n",
    "natdisTest = da.listeCorpus['natdis'].testDataSet\n",
    "\n",
    "\n",
    "for nameCorpus, corpus in da.listeCorpus.items():\n",
    "    if corpus.trainExist:\n",
    "        oovResult = corpus.computeCorpusOOV()\n",
    "        oovResultFoot = corpus.computeOOV(footTest)\n",
    "        oovResultnatdis = corpus.computeOOV(natdisTest)\n",
    "        print(nameCorpus, \" : \")\n",
    "        print(\"     Pourcentage de l'oov entre train et test : \", oovResult[0])\n",
    "        print(\"     Pourcentage de l'oov entre train et dev : \", oovResult[1])\n",
    "        print(\"     Pourcentage de l'oov entre train et footTest : \", oovResultFoot)\n",
    "        print(\"     Pourcentage de l'oov entre train et natdisTest : \", oovResultnatdis)\n",
    "        latexRows = (nameCorpus, oovResult[0], oovResult[1], oovResultFoot, oovResultnatdis)\n",
    "        oovLatex.append(latexRows)\n",
    "\n",
    "print(tabulate(oovLatex, tablefmt=\"latex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divergence de Kullback-Leibler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calcul des divergence de KullBack-Leibler\n",
    "\n",
    "latexDKL = []\n",
    "\n",
    "for nameCorpus, corpus in da.listeCorpus.items():\n",
    "    if corpus.trainExist:\n",
    "        print(nameCorpus + \" : \")\n",
    "        DKLResult = corpus.computeCorpusKLDivergence()\n",
    "        DKLFootresult = corpus.computeKLDivergence(footTest)\n",
    "        DKLnatdisResult = corpus.computeKLDivergence(natdisTest)\n",
    "        print(\"     Dkl(test||train) = \", DKLResult)\n",
    "        print(\"     Dkl(footTest||train) = \", DKLFootresult)\n",
    "        print(\"     Dkl(natdisTest||train) = \", DKLnatdisResult)\n",
    "        latexDKL.append( (nameCorpus, DKLResult, DKLFootresult, DKLnatdisResult) )\n",
    "        \n",
    "        \n",
    "print(tabulate(latexDKL, tablefmt=\"latex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latexPerplexity = []\n",
    "\n",
    "print(\"                         train,                   test,              dev\")\n",
    "for nameCorpus, corpus in da.listeCorpus.items():\n",
    "    print(nameCorpus, \" : \")\n",
    "    pp_res = corpus.computePerplexityCorpus()\n",
    "    print(\"     perplexity : \", pp_res)\n",
    "    latexRow = []\n",
    "    latexRow.append(nameCorpus)\n",
    "    if type(pp_res) == tuple:\n",
    "        for i in pp_res:\n",
    "            latexRow.append(i)\n",
    "    else:\n",
    "        latexRow.append(pp_res)\n",
    "    latexPerplexity.append( latexRow )\n",
    "    \n",
    "print(tabulate(latexPerplexity, tablefmt=\"latex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model\n",
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modelAnalysis as ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestTree, bestParam = ma.analyzeDecisionTree(\"partut\", maximum_depth=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bestParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23324, 19)\n",
      "(23324,)\n"
     ]
    }
   ],
   "source": [
    "import features as ft\n",
    "import DecisionTreeClassifier as dtree\n",
    "\n",
    "trainSet, testSet = da.listeCorpus[\"partut\"].trainDataSet, da.listeCorpus[\"partut\"].testDataSet\n",
    "\n",
    "Xtrain, Ytrain = ft.buildFeature(trainSet)\n",
    "Xtest, Ytest = ft.buildFeature(testSet)\n",
    "\n",
    "decisionTree = dtree.DecisionTreeClassifier(Xtrain, Ytrain, max_depth=16)\n",
    "\n",
    "dt2 = dtree.DecisionTreeClassifier(Xtest, Ytest, max_depth=16)\n",
    "\n",
    "print(np.shape(Xtrain))\n",
    "print(np.shape(Ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "tree = decisionTree.fit()\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreTest, cmTest = decisionTree.modelScore(tree, Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scoreTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma.showConfusionMatrix(cmTest, decisionTree.classLabel.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTreePartut.showTree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NaiveBayesClassifier as naivebc\n",
    "\n",
    "naiveBC = naivebc.NaiveBayesClassifier(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "naiveBC.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, cm = naiveBC.modelScore(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma.showConfusionMatrix(cm, decisionTree.classLabel.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=<dataAnalysis.DataSet object at 0x7ff151217438>.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-63c6136eaa5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestSet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \"\"\"\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_class_log_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    547\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=<dataAnalysis.DataSet object at 0x7ff151217438>.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "Xtrain, Ytrain = ft.buildFeature(trainSet, lambda x:x)\n",
    "Xtest, Ytest = ft.buildFeature(testSet, lambda x:x)\n",
    "\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(trainSet, testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
